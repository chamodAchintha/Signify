name: example_experiment
data:
  data_path: ./data/
  version: phoenix_2014_trans
  sgn: sign
  txt: text
  gls: gloss
  train: phoenix14t.pami0.train
  dev: phoenix14t.pami0.dev
  test: phoenix14t.pami0.test
  feature_size: 1024
  level: word
  txt_lowercase: true
  max_sent_length: 400
  random_train_subset: -1
  random_dev_subset: -1
  gls_vocab: ./data/gls.vocab
  txt_vocab: ./data/txt.vocab
  batch_size: 32
training:
  # load_model: "./SavedModels/example/13400.ckpt"
  random_seed: 44
  model_dir: "./SavedModels/example22"
  optimizer: adam
  learning_rate: 0.001
  batch_size: 32
  epochs: 500
  shuffle: true
  use_cuda: true
  betas:
    - 0.9
    - 0.998
  learning_rate_min: 0.00001
  patience: 6
  decrease_factor: 0.8
  # es_patience: 10
  # es_min_delta: 0
model:
  encoder:
    use_checkpoint: true
    checkpoint: ./output/best.ckpt
    skip_encoder: false
    type: transformer
    bayesian_attention: true
    bayesian_feedforward: true
    ibp: false
    activation: lwta
    lwta_competitors: 4
    num_layers: 2
    num_heads: 8
    embeddings:
      embedding_dim: 512
      scale: false
      bayesian: true
      ibp: false
      dropout: 0.2
      norm_type: batch
      activation_type: lwta
      lwta_competitors: 4
    hidden_size: 512
    ff_size: 2048
    dropout: 0.2
  classification_head:
    type: conv # mlp, attention, conv, rnn
    input_size: 3,
    num_classes: 900,
    kernel_size: 5, # if conv
    num_filters: 64, # if conv
    hidden_size: 128, # if mlp/rnn
    dropout_rate: 0.3, # if mlp
